{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa677d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a8400f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d552a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50fc7867",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'taylor_swift_data_kaggle/Albums/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04eee95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "albums = ['TaylorSwift',\n",
    "          'Fearless_TaylorsVersion_', \n",
    "          'SpeakNow_TaylorsVersion_',\n",
    "          'Red_TaylorsVersion_', \n",
    "          '1989_TaylorsVersion',  \n",
    "          'Reputation', \n",
    "          'Lover',\n",
    "          'Folklore',\n",
    "          'evermore_deluxeversion_', \n",
    "          'Midnights_TheTilDawnEdition_', \n",
    "          'THETORTUREDPOETSDEPARTMENT_THEANTHOLOGY'\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eaf56e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to extract from every album\n",
    "def create_df(file_path, album_name):\n",
    "    dir_path = f'{file_path}{album_name}'\n",
    "    data = [] # create empty dataframe for this album\n",
    "\n",
    "    # go through every file in this folder\n",
    "    for filename in os.listdir(dir_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            song_path = os.path.join(dir_path, filename) # file path for this song.txt\n",
    "\n",
    "        with open(song_path, 'r') as file:\n",
    "            file_content = file.read()\n",
    "\n",
    "        data.append({'album_title': album_name, 'song_title': filename, 'lyrics': file_content}) # create entry for song and lyrics observation\n",
    "\n",
    "    df = pd.DataFrame(data) # append to dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dfc641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for i in albums:\n",
    "    df = pd.concat([df, create_df(file_path, i)], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72f21b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['PicturetoBurn.txt', 'TimMcGraw.txt', 'TheOutside.txt',\n",
       "       'TeardropsOnMyGuitar.txt', 'MarysSong_OhMyMyMy_.txt',\n",
       "       'TiedTogetherwithaSmile.txt', 'APerfectlyGoodHeart.txt',\n",
       "       'APlaceInThisWorld.txt', 'ColdasYou.txt', 'OurSong.txt',\n",
       "       'StayBeautiful.txt', 'ImOnlyMeWhenImWithYou.txt', 'Invisible.txt',\n",
       "       'ShouldveSaidNo.txt', 'TeardropsonMyGuitar_PopVersion_.txt',\n",
       "       'TheWayILovedYou_TaylorsVersion_.txt',\n",
       "       'Fifteen_TaylorsVersion_.txt',\n",
       "       'TheOtherSideoftheDoor_TaylorsVersion_.txt',\n",
       "       'Mr.PerfectlyFine_TaylorsVersion__FromtheVault_.txt',\n",
       "       'JumpThenFall_TaylorsVersion_.txt',\n",
       "       'Superstar_TaylorsVersion_.txt',\n",
       "       'ByeByeBaby_TaylorsVersion__FromtheVault_.txt',\n",
       "       'YouAllOverMe_TaylorsVersion__FromtheVault_.txt',\n",
       "       'WeWereHappy_TaylorsVersion__FromtheVault_.txt',\n",
       "       'Untouchable_TaylorsVersion_.txt',\n",
       "       'HeyStephen_TaylorsVersion_.txt',\n",
       "       'IfThisWasaMovie_TaylorsVersion_.txt',\n",
       "       'ThatsWhen_TaylorsVersion__FromtheVault_.txt',\n",
       "       'TodayWasaFairytale_TaylorsVersion_.txt',\n",
       "       'Breathe_TaylorsVersion_.txt', 'LoveStory_TaylorsVersion_.txt',\n",
       "       \"You'reNotSorry_TaylorsVersion_.txt\",\n",
       "       'Forever_&_Always_TaylorsVersion_.txt',\n",
       "       'TellMeWhy_TaylorsVersion_.txt',\n",
       "       'YouBelongWithMe_TaylorsVersion_.txt',\n",
       "       'Forever_&_Always_PianoVersion__TaylorsVersion_.txt',\n",
       "       'WhiteHorse_TaylorsVersion_.txt',\n",
       "       'LoveStory_TaylorsVersion__ElviraRemix_.txt',\n",
       "       'Fearless_TaylorsVersion_.txt',\n",
       "       'ComeInWithTheRain_TaylorsVersion_.txt',\n",
       "       'TheBestDay_TaylorsVersion_.txt',\n",
       "       \"Don'tYou_TaylorsVersion__FromtheVault_.txt\",\n",
       "       'Change_TaylorsVersion_.txt', 'Enchanted_TaylorsVersion_.txt',\n",
       "       'Timeless_TaylorsVersion__FromTheVault_.txt',\n",
       "       'TheStoryofUs_TaylorsVersion_.txt',\n",
       "       'ICanSeeYou_TaylorsVersion__FromTheVault_.txt',\n",
       "       'SparksFly_TaylorsVersion_.txt', 'LastKiss_TaylorsVersion_.txt',\n",
       "       'Mean_TaylorsVersion_.txt',\n",
       "       'WhenEmmaFallsinLove_TaylorsVersion__FromTheVault_.txt',\n",
       "       'NeverGrowUp_TaylorsVersion_.txt', 'Mine_TaylorsVersion_.txt',\n",
       "       'BetterThanRevenge_TaylorsVersion_.txt',\n",
       "       'LongLive_TaylorsVersion_.txt', 'Superman_TaylorsVersion_.txt',\n",
       "       'Ours_TaylorsVersion_.txt', 'SpeakNow_TaylorsVersion_.txt',\n",
       "       'FoolishOne_TaylorsVersion__FromTheVault_.txt',\n",
       "       'DearJohn_TaylorsVersion_.txt',\n",
       "       'ElectricTouch_TaylorsVersion__FromTheVault_.txt',\n",
       "       'BackToDecember_TaylorsVersion_.txt',\n",
       "       'Innocent_TaylorsVersion_.txt', 'Haunted_TaylorsVersion_.txt',\n",
       "       'CastlesCrumbling_TaylorsVersion__FromTheVault_.txt',\n",
       "       'EyesOpen_TaylorsVersion_.txt', 'IAlmostDo_TaylorsVersion_.txt',\n",
       "       'Run_TaylorsVersion__FromtheVault_.txt', '22_TaylorsVersion_.txt',\n",
       "       'Ronan_TaylorsVersion_.txt', 'GirlAtHome_TaylorsVersion_.txt',\n",
       "       'StateOfGrace_AcousticVersion__TaylorsVersion_.txt',\n",
       "       'HolyGround_TaylorsVersion_.txt',\n",
       "       'ComeBack,_BeHere_TaylorsVersion_.txt',\n",
       "       'StayStayStay_TaylorsVersion_.txt',\n",
       "       'TheVeryFirstNight_TaylorsVersion__FromtheVault_.txt',\n",
       "       'WeAreNeverEverGettingBackTogether_TaylorsVersion_.txt',\n",
       "       'IBetYouThinkAboutMe_TaylorsVersion__FromtheVault_.txt',\n",
       "       'TheLuckyOne_TaylorsVersion_.txt',\n",
       "       'Treacherous_TaylorsVersion_.txt',\n",
       "       'NothingNew_TaylorsVersion__FromtheVault_.txt',\n",
       "       'BetterMan_TaylorsVersion__FromtheVault_.txt',\n",
       "       'Babe_TaylorsVersion__FromtheVault_.txt',\n",
       "       'Red_TaylorsVersion_.txt', 'AllTooWell_TaylorsVersion_.txt',\n",
       "       'TheLastTime_TaylorsVersion_.txt',\n",
       "       'BeginAgain_TaylorsVersion_.txt',\n",
       "       'SadBeautifulTragic_TaylorsVersion_.txt',\n",
       "       'Starlight_TaylorsVersion_.txt',\n",
       "       'AllTooWell_10MinuteVersion__TaylorsVersion__FromtheVault_.txt',\n",
       "       'Safe_&_Sound_TaylorsVersion_.txt',\n",
       "       'StateofGrace_TaylorsVersion_.txt',\n",
       "       'ForeverWinter_TaylorsVersion__FromtheVault_.txt',\n",
       "       'IKnewYouWereTrouble_TaylorsVersion_.txt',\n",
       "       'TheMomentIKnew_TaylorsVersion_.txt',\n",
       "       'MessageInABottle_TaylorsVersion__FromtheVault_.txt',\n",
       "       'EverythingHasChanged_TaylorsVersion_.txt',\n",
       "       'SuburbanLegends_TaylorsVersion__FromTheVault_.txt',\n",
       "       'WildestDreams_TaylorsVersion_.txt',\n",
       "       'AllYouHadToDoWasStay_TaylorsVersion_.txt',\n",
       "       'BadBlood_Remix__TaylorsVersion_.txt',\n",
       "       'NewRomantics_TaylorsVersion_.txt',\n",
       "       'OutOfTheWoods_TaylorsVersion_.txt',\n",
       "       'Wonderland_TaylorsVersion_.txt',\n",
       "       'NowThatWeDontTalk_TaylorsVersion__FromTheVault_.txt',\n",
       "       'IKnowPlaces_TaylorsVersion_.txt',\n",
       "       'ShakeItOff_TaylorsVersion_.txt',\n",
       "       \"'Slut!'_TaylorsVersion__FromTheVault_.txt\",\n",
       "       'SweeterThanFiction_TaylorsVersion_.txt',\n",
       "       'ThisLove_TaylorsVersion_.txt', 'Clean_TaylorsVersion_.txt',\n",
       "       'YouAreInLove_TaylorsVersion_.txt', 'Style_TaylorsVersion_.txt',\n",
       "       'BlankSpace_TaylorsVersion_.txt', 'BadBlood_TaylorsVersion_.txt',\n",
       "       'WelcomeToNewYork_TaylorsVersion_.txt',\n",
       "       'IsItOverNow__TaylorsVersion__FromTheVault_.txt',\n",
       "       'IWishYouWould_TaylorsVersion_.txt',\n",
       "       'SayDontGo_TaylorsVersion__FromTheVault_.txt',\n",
       "       'HowYouGetTheGirl_TaylorsVersion_.txt', 'IDidSomethingBad.txt',\n",
       "       'Delicate.txt', 'DontBlameMe.txt', 'LookWhatYouMadeMeDo.txt',\n",
       "       'EndGame.txt', '...ReadyforIt?.txt', 'CallItWhatYouWant.txt',\n",
       "       'NewYearsDay.txt', 'Gorgeous.txt', 'GetawayCar.txt', 'Dress.txt',\n",
       "       'KingofMyHeart.txt', 'DancingWithOurHandsTied.txt',\n",
       "       'SoItGoes....txt', 'ThisIsWhyWeCantHaveNiceThings.txt',\n",
       "       'ItsNicetoHaveaFriend.txt', \"SoonYou'llGetBetter.txt\", 'ME_.txt',\n",
       "       'CorneliaStreet.txt', 'YouNeedToCalmDown.txt', 'TheMan.txt',\n",
       "       'Lover.txt', 'TheArcher.txt', 'DeathbyaThousandCuts.txt',\n",
       "       'PaperRings.txt', 'Afterglow.txt',\n",
       "       'AllOfTheGirlsYouLovedBefore.txt', 'IForgotThatYouExisted.txt',\n",
       "       'Daylight.txt', 'CruelSummer.txt', 'LondonBoy.txt', 'FalseGod.txt',\n",
       "       'MissAmericana_&_TheHeartbreakPrince.txt', 'IThinkHeKnows.txt',\n",
       "       'thelastgreatamericandynasty.txt', 'thisismetrying.txt',\n",
       "       'epiphany.txt', 'mytearsricochet.txt', 'exile.txt',\n",
       "       'mirrorball.txt', 'seven.txt', 'peace.txt', 'the1.txt',\n",
       "       'cardigan.txt', 'thelakes_originalversion_.txt', 'betty.txt',\n",
       "       'Carolina.txt', 'invisiblestring.txt', 'august.txt',\n",
       "       'thelakes.txt', 'hoax.txt', 'madwoman.txt', 'illicitaffairs.txt',\n",
       "       'tisthedamnseason.txt', 'evermore.txt', 'happiness.txt',\n",
       "       'tolerateit.txt', 'willow.txt', 'marjorie.txt', 'itstimetogo.txt',\n",
       "       'ivy.txt', 'dorothea.txt', 'closure.txt', 'cowboylikeme.txt',\n",
       "       'longstoryshort.txt', 'goldrush.txt', 'rightwhereyouleftme.txt',\n",
       "       'nobody_nocrime.txt', 'champagneproblems.txt', 'coneyisland.txt',\n",
       "       'MidnightRain.txt', 'Maroon.txt', 'BiggerThanTheWholeSky.txt',\n",
       "       'Labyrinth.txt', 'Anti-Hero.txt', 'TheGreatWar.txt',\n",
       "       'SnowOnTheBeach.txt', 'Glitch.txt', 'Paris.txt', 'DearReader.txt',\n",
       "       'Karma_Remix_.txt', 'VigilanteShit.txt', 'Karma.txt',\n",
       "       \"Would've_Could've_Should've.txt\", 'Question...?.txt',\n",
       "       'SweetNothing.txt', 'Bejeweled.txt', 'HitsDifferent.txt',\n",
       "       \"You'reLosingMe.txt\", 'Mastermind.txt', 'HighInfidelity.txt',\n",
       "       \"You'reOnYourOwn_Kid.txt\", 'LavenderHaze.txt',\n",
       "       'SnowOnTheBeach_feat_MoreLanaDelRey_.txt', 'HowDidItEnd_.txt',\n",
       "       'TheBolter.txt', 'Peter.txt', 'imgonnagetyouback.txt',\n",
       "       'DownBad.txt', 'ICanDoItWithABrokenHeart.txt', 'GuiltyasSin_.txt',\n",
       "       'TheBlackDog.txt', 'IHateItHere.txt', 'Robin.txt',\n",
       "       'ChloeorSamorSophiaorMarcus.txt', 'ButDaddyILoveHim.txt',\n",
       "       'ClaraBow.txt', 'loml.txt', 'TheProphecy.txt',\n",
       "       'ILookinPeoplesWindows.txt', 'TheManuscript.txt',\n",
       "       'thanKyouaIMee.txt', 'SoHighSchool.txt', 'TheAlchemy.txt',\n",
       "       'Florida!!!.txt', 'MyBoyOnlyBreaksHisFavoriteToys.txt',\n",
       "       'TheAlbatross.txt', 'TheTorturedPoetsDepartment.txt',\n",
       "       'Cassandra.txt', 'SoLong_London.txt',\n",
       "       'ICanFixHim_NoReallyICan_.txt', 'WhosAfraidofLittleOldMe_.txt',\n",
       "       'Fortnight.txt', 'FreshOutTheSlammer.txt',\n",
       "       'TheSmallestManWhoEverLived.txt'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['song_title'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da0cef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # 1. Remove everything before and including the word \"Lyrics\"\n",
    "    text = re.sub(r\".*?Lyrics\", \"\", text)\n",
    "    \n",
    "    # 2. Remove any number (if present) followed by \"Embed\" at the end\n",
    "    text = re.sub(r\"\\s*\\d*\\s*Embed\\s*$\", \"\", text)\n",
    "    \n",
    "    # 3. Replace newlines with spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 4. Remove the extra slashes from quotes in songs\n",
    "    text = re.sub(r\"\\'\", \"\", text)\n",
    "    \n",
    "    # 5. Remove the [Intro], [Chorus], etc. informaiton\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "\n",
    "    # 6. Unwanted characters\n",
    "    text = re.sub(r'See Taylor Swift LiveGet tickets as low as \\$60You might also like', '', text)\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a3c4f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_title(text):\n",
    "    # 1. Replace _**_ with (**)\n",
    "    text = re.sub(r'_(.*?)_', r' (\\1)', text)\n",
    "\n",
    "    # 2. Remove the .txt \n",
    "    text = re.sub('.txt', '', text)\n",
    "\n",
    "    # 3. Fix formatting\n",
    "    text = re.sub(r\"TaylorsVersion\", \"Taylor's Version\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
    "    text = re.sub(r\"Fromthe Vault\", \"From The Vault\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(r'_', r' ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcb5bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize is not as formulaic as stem \n",
    "# (so it takes in more consideration of the word rather than just stemming to the root)\n",
    "def lemmatize_text(text):\n",
    "    return ' '.join(lemmatizer.lemmatize(w) for w in (re.sub(',', '', text).lower()).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d16a9034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean dataframe\n",
    "df['lyrics'] = df['lyrics'].apply(clean_text)\n",
    "df['song_title'] = df['song_title'].apply(clean_title)\n",
    "df['lemmatize_lyrics'] = df['lyrics'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5298151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['TaylorSwift', 'Fearless_TaylorsVersion_',\n",
       "       'SpeakNow_TaylorsVersion_', 'Red_TaylorsVersion_',\n",
       "       '1989_TaylorsVersion', 'Reputation', 'Lover', 'Folklore',\n",
       "       'evermore_deluxeversion_', 'Midnights_TheTilDawnEdition_',\n",
       "       'THETORTUREDPOETSDEPARTMENT_THEANTHOLOGY'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['album_title'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8071ccab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>album_title</th>\n",
       "      <th>song_title</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>lemmatize_lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TaylorSwift</td>\n",
       "      <td>Pictureto Burn</td>\n",
       "      <td>State the obvious, I didnt get my perfect fant...</td>\n",
       "      <td>state the obvious i didnt get my perfect fanta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TaylorSwift</td>\n",
       "      <td>Tim Mc Graw</td>\n",
       "      <td>He said the way my blue eyes shined Put those ...</td>\n",
       "      <td>he said the way my blue eye shined put those g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TaylorSwift</td>\n",
       "      <td>The Outside</td>\n",
       "      <td>I didnt know what I would find When I went loo...</td>\n",
       "      <td>i didnt know what i would find when i went loo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TaylorSwift</td>\n",
       "      <td>Teardrops On My Guitar</td>\n",
       "      <td>Drew looks at me I fake a smile, so he wont se...</td>\n",
       "      <td>drew look at me i fake a smile so he wont see ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TaylorSwift</td>\n",
       "      <td>Marys Song (Oh My My My)</td>\n",
       "      <td>She said I was seven and you were nine I looke...</td>\n",
       "      <td>she said i wa seven and you were nine i looked...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>THETORTUREDPOETSDEPARTMENT_THEANTHOLOGY</td>\n",
       "      <td>ICan Fix Him (No Really ICan)</td>\n",
       "      <td>The smoke cloud billows out his mouth Like a f...</td>\n",
       "      <td>the smoke cloud billow out his mouth like a fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>THETORTUREDPOETSDEPARTMENT_THEANTHOLOGY</td>\n",
       "      <td>Whos Afraidof Little Old Me</td>\n",
       "      <td>The whos who of \"Whos that?\" is poised for the...</td>\n",
       "      <td>the who who of \"whos that?\" is poised for the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>THETORTUREDPOETSDEPARTMENT_THEANTHOLOGY</td>\n",
       "      <td>Fortnight</td>\n",
       "      <td>I was supposed to be sent away But they forgot...</td>\n",
       "      <td>i wa supposed to be sent away but they forgot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>THETORTUREDPOETSDEPARTMENT_THEANTHOLOGY</td>\n",
       "      <td>Fresh Out The Slammer</td>\n",
       "      <td>Now, pretty baby, Im runnin back home to you F...</td>\n",
       "      <td>now pretty baby im runnin back home to you fre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>THETORTUREDPOETSDEPARTMENT_THEANTHOLOGY</td>\n",
       "      <td>The Smallest Man Who Ever Lived</td>\n",
       "      <td>Was any of it true? Gazing at me starry-eyed I...</td>\n",
       "      <td>wa any of it true? gazing at me starry-eyed in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>245 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 album_title                       song_title  \\\n",
       "0                                TaylorSwift                   Pictureto Burn   \n",
       "1                                TaylorSwift                      Tim Mc Graw   \n",
       "2                                TaylorSwift                      The Outside   \n",
       "3                                TaylorSwift           Teardrops On My Guitar   \n",
       "4                                TaylorSwift         Marys Song (Oh My My My)   \n",
       "..                                       ...                              ...   \n",
       "240  THETORTUREDPOETSDEPARTMENT_THEANTHOLOGY    ICan Fix Him (No Really ICan)   \n",
       "241  THETORTUREDPOETSDEPARTMENT_THEANTHOLOGY     Whos Afraidof Little Old Me    \n",
       "242  THETORTUREDPOETSDEPARTMENT_THEANTHOLOGY                        Fortnight   \n",
       "243  THETORTUREDPOETSDEPARTMENT_THEANTHOLOGY            Fresh Out The Slammer   \n",
       "244  THETORTUREDPOETSDEPARTMENT_THEANTHOLOGY  The Smallest Man Who Ever Lived   \n",
       "\n",
       "                                                lyrics  \\\n",
       "0    State the obvious, I didnt get my perfect fant...   \n",
       "1    He said the way my blue eyes shined Put those ...   \n",
       "2    I didnt know what I would find When I went loo...   \n",
       "3    Drew looks at me I fake a smile, so he wont se...   \n",
       "4    She said I was seven and you were nine I looke...   \n",
       "..                                                 ...   \n",
       "240  The smoke cloud billows out his mouth Like a f...   \n",
       "241  The whos who of \"Whos that?\" is poised for the...   \n",
       "242  I was supposed to be sent away But they forgot...   \n",
       "243  Now, pretty baby, Im runnin back home to you F...   \n",
       "244  Was any of it true? Gazing at me starry-eyed I...   \n",
       "\n",
       "                                      lemmatize_lyrics  \n",
       "0    state the obvious i didnt get my perfect fanta...  \n",
       "1    he said the way my blue eye shined put those g...  \n",
       "2    i didnt know what i would find when i went loo...  \n",
       "3    drew look at me i fake a smile so he wont see ...  \n",
       "4    she said i wa seven and you were nine i looked...  \n",
       "..                                                 ...  \n",
       "240  the smoke cloud billow out his mouth like a fr...  \n",
       "241  the who who of \"whos that?\" is poised for the ...  \n",
       "242  i wa supposed to be sent away but they forgot ...  \n",
       "243  now pretty baby im runnin back home to you fre...  \n",
       "244  wa any of it true? gazing at me starry-eyed in...  \n",
       "\n",
       "[245 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "636fc96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('songs_lyrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f7075fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipline to chain multiple steps in a workflow (recipe)\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1,1))), # feature extraction: transforms text into bag of words model\n",
    "    ('tfidf', TfidfTransformer()), # tf-idf: transfroms the count matrix into tf-idf\n",
    "    ('clf', RandomForestClassifier()) # ML: random forest to classify with decission trees \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "815d09b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to find search recommendations\n",
    "def get_results(user_input):\n",
    "    # determine the number of words in user search\n",
    "    search_length = user_input.split()\n",
    "    n_words = len(search_length)\n",
    "\n",
    "    # if user is looking for phrase (trigrams to total word-grams)\n",
    "    if n_words > 2:\n",
    "        tfv = TfidfVectorizer(ngram_range=(3, n_words), use_idf=False)\n",
    "        tfv_matrix = tfv.fit_transform(df['lemmatize_lyrics'])\n",
    "\n",
    "    # if user is just searching for words (unigrams, bigrams)\n",
    "    else:\n",
    "        tfv = TfidfVectorizer(ngram_range=(1, n_words), use_idf=False)\n",
    "        tfv_matrix = tfv.fit_transform(df['lemmatize_lyrics'])\n",
    "    \n",
    "    # fit user input \n",
    "    tfv_user = tfv.transform([lemmatize_text(user_input)])\n",
    "\n",
    "    # calculate similarity scores between search and all songs\n",
    "    cosine_similarities = cosine_similarity(tfv_user, tfv_matrix)\n",
    "    cosine_similarities = cosine_similarities.flatten()\n",
    "\n",
    "    # create matrix for all songs that match \n",
    "    matches = pd.DataFrame()\n",
    "    matches['index'] = np.where(cosine_similarities > 0)[0]\n",
    "    matches['score'] = cosine_similarities[matches]\n",
    "    matches_sorted = matches.sort_values(by='score', ascending=False)\n",
    "\n",
    "    # output the list of similarities\n",
    "    for i in range(len(matches_sorted)):\n",
    "        song_index = matches_sorted['index'].iloc[i]\n",
    "        print(f\"Song: {df['song_title'][song_index]}, Similarity Score: {matches_sorted['score'].iloc[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "dbcaddac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song: ICanDoItWithABrokenHeart, Similarity Score: 0.0379\n",
      "Song: MyBoyOnlyBreaksHisFavoriteToys, Similarity Score: 0.0169\n",
      "Song: YoureOnYourOwn_Kid, Similarity Score: 0.0085\n"
     ]
    }
   ],
   "source": [
    "get_results(\"He said he'd love me all his life\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "9eec5f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song: ClaraBow, Similarity Score: 0.0523\n",
      "Song: 22(TaylorsVersion), Similarity Score: 0.0228\n",
      "Song: BeginAgain(TaylorsVersion), Similarity Score: 0.0122\n",
      "Song: ()(ReadyforIt), Similarity Score: 0.0082\n",
      "Song: LookWhatYouMadeMeDo, Similarity Score: 0.0040\n"
     ]
    }
   ],
   "source": [
    "get_results(\"taylor swift\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "601bc8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song: MidnightRain, Similarity Score: 0.2084\n",
      "Song: NewYearsDay, Similarity Score: 0.0498\n",
      "Song: happiness, Similarity Score: 0.0402\n",
      "Song: LavenderHaze, Similarity Score: 0.0268\n",
      "Song: thelastgreatamericandynasty, Similarity Score: 0.0259\n",
      "Song: Paris, Similarity Score: 0.0247\n",
      "Song: Anti_Hero, Similarity Score: 0.0221\n",
      "Song: Style(TaylorsVersion), Similarity Score: 0.0185\n",
      "Song: 22(TaylorsVersion), Similarity Score: 0.0159\n",
      "Song: YouAreInLove(TaylorsVersion), Similarity Score: 0.0128\n"
     ]
    }
   ],
   "source": [
    "get_results(\"midnight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "46a3e84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song: SoItGoes()_, Similarity Score: 0.3835\n",
      "Song: TheVeryFirstNight(TaylorsVersion)(FromtheVault), Similarity Score: 0.0302\n",
      "Song: Style(TaylorsVersion), Similarity Score: 0.0288\n",
      "Song: YouAreInLove(TaylorsVersion), Similarity Score: 0.0239\n"
     ]
    }
   ],
   "source": [
    "get_results(\"so it goes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "50c31c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song: Safe(Sound)TaylorsVersion_, Similarity Score: 0.1002\n",
      "Song: Treacherous(TaylorsVersion), Similarity Score: 0.0499\n",
      "Song: loml, Similarity Score: 0.0255\n",
      "Song: SoLong_London, Similarity Score: 0.0248\n",
      "Song: Wouldve(Couldve)Shouldve, Similarity Score: 0.0179\n",
      "Song: DownBad, Similarity Score: 0.0164\n"
     ]
    }
   ],
   "source": [
    "get_results(\"safe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1a0d4cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.5) # each are just subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "db01f480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf', RandomForestClassifier())])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train to predict album based on the lemmatized lyrics using .fit(X, Y)\n",
    "text_clf.fit(df_train['lemmatize_lyrics'], df_train['album_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "29164830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['begging', 'begin', 'beginning', 'bein', 'being', 'believe',\n",
       "       'believed', 'believing', 'belong', 'belt'], dtype=object)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf['vect'].get_feature_names_out()[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "32d24cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1540"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_clf['vect'].get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f55024b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.97041447, 1.8303483 , 3.97041447, ..., 3.97041447, 3.97041447,\n",
       "       3.05412373])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the computed IDF values for each observation in training\n",
    "text_clf['tfidf'].idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f512712c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.00498709, 0.        , ..., 0.        , 0.0010381 ,\n",
       "       0.00120001])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an estimate of the importance of each feature used by the classifier (ie weight of each word)\n",
    "text_clf['clf'].feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1c9a8fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>were</td>\n",
       "      <td>0.017177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>just</td>\n",
       "      <td>0.016744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>my</td>\n",
       "      <td>0.016372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1529</th>\n",
       "      <td>you</td>\n",
       "      <td>0.014092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>for</td>\n",
       "      <td>0.013315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>landing</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>land</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>lady</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>knеw</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2am</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1540 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature  importance\n",
       "1449     were    0.017177\n",
       "692      just    0.016744\n",
       "859        my    0.016372\n",
       "1529      you    0.014092\n",
       "479       for    0.013315\n",
       "...       ...         ...\n",
       "714   landing    0.000000\n",
       "713      land    0.000000\n",
       "712      lady    0.000000\n",
       "711      knеw    0.000000\n",
       "0         2am    0.000000\n",
       "\n",
       "[1540 rows x 2 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"feature\": text_clf['vect'].get_feature_names_out(), \n",
    "                           'importance': text_clf['clf'].feature_importances_}).sort_values(by=['importance'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3aba6a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = text_clf.predict(df_test['lemmatize_lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "eee45723",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = text_clf.predict_proba(df_test[text_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0d339b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['pred'] = y_pred\n",
    "df_test['proba'] = [item[1] for item in y_proba]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7e7173ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(ngram_range = (1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f5bf0e2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<76x2222 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9618 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfv.fit_transform(df['lyrics'].apply(lambda x: re.sub(',', '', x).lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "00eaebe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2am',\n",
       " '45',\n",
       " '90s',\n",
       " 'abigail',\n",
       " 'about',\n",
       " 'above',\n",
       " 'absent',\n",
       " 'accident',\n",
       " 'account',\n",
       " 'ache',\n",
       " 'achievement',\n",
       " 'achilles',\n",
       " 'aching',\n",
       " 'acid',\n",
       " 'across',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'actin',\n",
       " 'actress',\n",
       " 'actually',\n",
       " 'addressed',\n",
       " 'admitted',\n",
       " 'affair',\n",
       " 'affairyou',\n",
       " 'affection',\n",
       " 'afraid',\n",
       " 'after',\n",
       " 'again',\n",
       " 'age',\n",
       " 'ago',\n",
       " 'ah',\n",
       " 'ain',\n",
       " 'aint',\n",
       " 'air',\n",
       " 'airport',\n",
       " 'album',\n",
       " 'align',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'alley',\n",
       " 'alls',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'always',\n",
       " 'am',\n",
       " 'amber',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'angel',\n",
       " 'angels',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'anticipatin',\n",
       " 'any',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'apologies',\n",
       " 'apology',\n",
       " 'applauded',\n",
       " 'arcade',\n",
       " 'are',\n",
       " 'arent',\n",
       " 'arm',\n",
       " 'armor',\n",
       " 'arms',\n",
       " 'army',\n",
       " 'around',\n",
       " 'art',\n",
       " 'arе',\n",
       " 'as',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'askin',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'asleep',\n",
       " 'assume',\n",
       " 'at',\n",
       " 'attached',\n",
       " 'attention',\n",
       " 'attitude',\n",
       " 'autumn',\n",
       " 'awake',\n",
       " 'away',\n",
       " 'babe',\n",
       " 'baby',\n",
       " 'babylon',\n",
       " 'back',\n",
       " 'backlogged',\n",
       " 'bad',\n",
       " 'bait',\n",
       " 'balcony',\n",
       " 'ball',\n",
       " 'bandit',\n",
       " 'bar',\n",
       " 'barbed',\n",
       " 'bare',\n",
       " 'barefoot',\n",
       " 'barren',\n",
       " 'bars',\n",
       " 'bathroom',\n",
       " 'battle',\n",
       " 'battles',\n",
       " 'be',\n",
       " 'bear',\n",
       " 'beat',\n",
       " 'beating',\n",
       " 'beautiful',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becoming',\n",
       " 'bed',\n",
       " 'bedroom',\n",
       " 'beds',\n",
       " 'bedtime',\n",
       " 'been',\n",
       " 'beers',\n",
       " 'before',\n",
       " 'beg',\n",
       " 'begged',\n",
       " 'beggin',\n",
       " 'begging',\n",
       " 'begin',\n",
       " 'beginning',\n",
       " 'behind',\n",
       " 'bein',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'believed',\n",
       " 'believing',\n",
       " 'belong',\n",
       " 'belt',\n",
       " 'bench',\n",
       " 'beneath',\n",
       " 'bent',\n",
       " 'best',\n",
       " 'bet',\n",
       " 'better',\n",
       " 'bettеr',\n",
       " 'between',\n",
       " 'beverly',\n",
       " 'beyond',\n",
       " 'biding',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'bills',\n",
       " 'birthday',\n",
       " 'black',\n",
       " 'blame',\n",
       " 'blankets',\n",
       " 'blaze',\n",
       " 'bleachers',\n",
       " 'bled',\n",
       " 'bleeds',\n",
       " 'blew',\n",
       " 'blind',\n",
       " 'blink',\n",
       " 'block',\n",
       " 'blood',\n",
       " 'blooms',\n",
       " 'blow',\n",
       " 'blue',\n",
       " 'blues',\n",
       " 'bluest',\n",
       " 'blush',\n",
       " 'boardwalk',\n",
       " 'boating',\n",
       " 'boats',\n",
       " 'bobby',\n",
       " 'body',\n",
       " 'bomb',\n",
       " 'bone',\n",
       " 'bones',\n",
       " 'book',\n",
       " 'booked',\n",
       " 'books',\n",
       " 'boots',\n",
       " 'born',\n",
       " 'borrowed',\n",
       " 'boss',\n",
       " 'both',\n",
       " 'bottle',\n",
       " 'bottoms',\n",
       " 'bought',\n",
       " 'bout',\n",
       " 'boy',\n",
       " 'boys',\n",
       " 'brand',\n",
       " 'brave',\n",
       " 'bravest',\n",
       " 'break',\n",
       " 'breakdown',\n",
       " 'breakfast',\n",
       " 'breakin',\n",
       " 'breaking',\n",
       " 'breaks',\n",
       " 'breakups',\n",
       " 'breath',\n",
       " 'breathe',\n",
       " 'breathing',\n",
       " 'bride',\n",
       " 'bright',\n",
       " 'brighter',\n",
       " 'bring',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'brokеn',\n",
       " 'brooklyn',\n",
       " 'brother',\n",
       " 'brought',\n",
       " 'bruise',\n",
       " 'brush',\n",
       " 'building',\n",
       " 'built',\n",
       " 'bulletproof',\n",
       " 'bump',\n",
       " 'bunch',\n",
       " 'buried',\n",
       " 'burn',\n",
       " 'burned',\n",
       " 'burning',\n",
       " 'burst',\n",
       " 'bushes',\n",
       " 'business',\n",
       " 'bustling',\n",
       " 'busy',\n",
       " 'but',\n",
       " 'butterflies',\n",
       " 'buy',\n",
       " 'by',\n",
       " 'bye',\n",
       " 'byline',\n",
       " 'bе',\n",
       " 'bеin',\n",
       " 'bеlong',\n",
       " 'cab',\n",
       " 'cabin',\n",
       " 'cabs',\n",
       " 'café',\n",
       " 'cake',\n",
       " 'call',\n",
       " 'called',\n",
       " 'calls',\n",
       " 'calmer',\n",
       " 'came',\n",
       " 'camera',\n",
       " 'can',\n",
       " 'candles',\n",
       " 'cannonball',\n",
       " 'cant',\n",
       " 'captain',\n",
       " 'capture',\n",
       " 'car',\n",
       " 'care',\n",
       " 'careless',\n",
       " 'cares',\n",
       " 'carried',\n",
       " 'carry',\n",
       " 'cars',\n",
       " 'case',\n",
       " 'casually',\n",
       " 'catch',\n",
       " 'catching',\n",
       " 'caught',\n",
       " 'cause',\n",
       " 'causе',\n",
       " 'celebrated',\n",
       " 'celebrating',\n",
       " 'centerfold',\n",
       " 'certain',\n",
       " 'chain',\n",
       " 'chair',\n",
       " 'champagne',\n",
       " 'champion',\n",
       " 'champions',\n",
       " 'chance',\n",
       " 'chances',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'changing',\n",
       " 'changеd',\n",
       " 'charmed',\n",
       " 'charming',\n",
       " 'chase',\n",
       " 'chasing',\n",
       " 'chatter',\n",
       " 'cheat',\n",
       " 'check',\n",
       " 'checking',\n",
       " 'cheeks',\n",
       " 'cheer',\n",
       " 'chest',\n",
       " 'chevy',\n",
       " 'child',\n",
       " 'children',\n",
       " 'chill',\n",
       " 'choice',\n",
       " 'choices',\n",
       " 'choose',\n",
       " 'chose',\n",
       " 'christmas',\n",
       " 'circles',\n",
       " 'circus',\n",
       " 'cities',\n",
       " 'city',\n",
       " 'citys',\n",
       " 'class',\n",
       " 'clean',\n",
       " 'cleaned',\n",
       " 'clear',\n",
       " 'clearer',\n",
       " 'clever',\n",
       " 'cliff',\n",
       " 'climbed',\n",
       " 'clock',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'closer',\n",
       " 'closet',\n",
       " 'closets',\n",
       " 'closure',\n",
       " 'closureyou',\n",
       " 'cloth',\n",
       " 'clothes',\n",
       " 'clover',\n",
       " 'club',\n",
       " 'clung',\n",
       " 'coast',\n",
       " 'coastal',\n",
       " 'coat',\n",
       " 'coaxed',\n",
       " 'code',\n",
       " 'coffee',\n",
       " 'cold',\n",
       " 'colder',\n",
       " 'collected',\n",
       " 'colors',\n",
       " 'comb',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'comet',\n",
       " 'comfortable',\n",
       " 'comin',\n",
       " 'coming',\n",
       " 'community',\n",
       " 'complained',\n",
       " 'complicated',\n",
       " 'con',\n",
       " 'concerts',\n",
       " 'coney',\n",
       " 'confess',\n",
       " 'confused',\n",
       " 'consequence',\n",
       " 'consider',\n",
       " 'contrarian',\n",
       " 'conversation',\n",
       " 'cool',\n",
       " 'cooler',\n",
       " 'core',\n",
       " 'corner',\n",
       " 'cornered',\n",
       " 'cost',\n",
       " 'costs',\n",
       " 'couch',\n",
       " 'could',\n",
       " 'couldnt',\n",
       " 'couldve',\n",
       " 'count',\n",
       " 'counter',\n",
       " 'counting',\n",
       " 'county',\n",
       " 'court',\n",
       " 'cover',\n",
       " 'covered',\n",
       " 'cowboy',\n",
       " 'cracks',\n",
       " 'crawling',\n",
       " 'crazy',\n",
       " 'creaking',\n",
       " 'creeping',\n",
       " 'crept',\n",
       " 'crescent',\n",
       " 'crestfallen',\n",
       " 'cried',\n",
       " 'cries',\n",
       " 'crime',\n",
       " 'criticize',\n",
       " 'crook',\n",
       " 'cross',\n",
       " 'crossed',\n",
       " 'crossing',\n",
       " 'crossword',\n",
       " 'crowd',\n",
       " 'crowded',\n",
       " 'crowds',\n",
       " 'cruel',\n",
       " 'cruelest',\n",
       " 'crumpled',\n",
       " 'crush',\n",
       " 'crust',\n",
       " 'cry',\n",
       " 'cryin',\n",
       " 'crying',\n",
       " 'cure',\n",
       " 'current',\n",
       " 'cursed',\n",
       " 'curses',\n",
       " 'cursing',\n",
       " 'curtained',\n",
       " 'cut',\n",
       " 'cute',\n",
       " 'cuts',\n",
       " 'dad',\n",
       " 'daddy',\n",
       " 'daddys',\n",
       " 'dagger',\n",
       " 'damage',\n",
       " 'damn',\n",
       " 'damsel',\n",
       " 'dance',\n",
       " 'dancin',\n",
       " 'dancing',\n",
       " 'dangerous',\n",
       " 'dappled',\n",
       " 'dare',\n",
       " 'dark',\n",
       " 'darkest',\n",
       " 'darling',\n",
       " 'dash',\n",
       " 'date',\n",
       " 'dated',\n",
       " 'datin',\n",
       " 'day',\n",
       " 'daydream',\n",
       " 'days',\n",
       " 'dead',\n",
       " 'deadlines',\n",
       " 'death',\n",
       " 'deceiving',\n",
       " 'december',\n",
       " 'decide',\n",
       " 'deck',\n",
       " 'deep',\n",
       " 'deepest',\n",
       " 'defeat',\n",
       " 'delicate',\n",
       " 'delusion',\n",
       " 'demons',\n",
       " 'deserved',\n",
       " 'desperately',\n",
       " 'devastated',\n",
       " 'diamond',\n",
       " 'diamonds',\n",
       " 'did',\n",
       " 'didnt',\n",
       " 'die',\n",
       " 'died',\n",
       " 'different',\n",
       " 'difficult',\n",
       " 'dignified',\n",
       " 'dignity',\n",
       " 'dim',\n",
       " 'dimming',\n",
       " 'dinner',\n",
       " 'dinosaurs',\n",
       " 'disappear',\n",
       " 'disappeared',\n",
       " 'disappearing',\n",
       " 'disappointments',\n",
       " 'disarm',\n",
       " 'disaster',\n",
       " 'disbelief',\n",
       " 'disposition',\n",
       " 'distance',\n",
       " 'distant',\n",
       " 'distress',\n",
       " 'ditch',\n",
       " 'divide',\n",
       " 'dividin',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesnt',\n",
       " 'dog',\n",
       " 'doin',\n",
       " 'doing',\n",
       " 'dollar',\n",
       " 'dom',\n",
       " 'dominoes',\n",
       " 'don',\n",
       " 'done',\n",
       " 'dont',\n",
       " 'door',\n",
       " 'doors',\n",
       " 'doorway',\n",
       " 'dorm',\n",
       " 'dorothea',\n",
       " 'double',\n",
       " 'doubt',\n",
       " 'doubted',\n",
       " 'down',\n",
       " 'downs',\n",
       " 'drag',\n",
       " 'dramatic',\n",
       " 'drawer',\n",
       " 'drawing',\n",
       " 'dream',\n",
       " 'dreamed',\n",
       " 'dreamer',\n",
       " 'dreamin',\n",
       " 'dreaming',\n",
       " 'dreamland',\n",
       " 'dreams',\n",
       " 'dress',\n",
       " 'dressed',\n",
       " 'dresser',\n",
       " 'dried',\n",
       " 'drink',\n",
       " 'drive',\n",
       " 'drivin',\n",
       " 'driving',\n",
       " 'drop',\n",
       " 'dropped',\n",
       " 'drowning',\n",
       " 'duchess',\n",
       " 'dude',\n",
       " 'dust',\n",
       " 'dwarves',\n",
       " 'each',\n",
       " 'eagles',\n",
       " 'ear',\n",
       " 'early',\n",
       " 'earned',\n",
       " 'easy',\n",
       " 'echoed',\n",
       " 'echoes',\n",
       " 'edge',\n",
       " 'effacing',\n",
       " 'eight',\n",
       " 'eighteen',\n",
       " 'either',\n",
       " 'else',\n",
       " 'em',\n",
       " 'empty',\n",
       " 'end',\n",
       " 'endearing',\n",
       " 'ending',\n",
       " 'endings',\n",
       " 'endless',\n",
       " 'ends',\n",
       " 'enough',\n",
       " 'erasing',\n",
       " 'escape',\n",
       " 'escaped',\n",
       " 'este',\n",
       " 'estes',\n",
       " 'euphoric',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'evergreen',\n",
       " 'evermore',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everybodys',\n",
       " 'everyone',\n",
       " 'everythin',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'evеr',\n",
       " 'ew',\n",
       " 'exactly',\n",
       " 'excellent',\n",
       " 'exception',\n",
       " 'exes',\n",
       " 'exhausting',\n",
       " 'expected',\n",
       " 'expecting',\n",
       " 'eye',\n",
       " 'eyed',\n",
       " 'eyes',\n",
       " 'face',\n",
       " 'fact',\n",
       " 'facts',\n",
       " 'fade',\n",
       " 'faded',\n",
       " 'fades',\n",
       " 'fading',\n",
       " 'fair',\n",
       " 'fairytale',\n",
       " 'faith',\n",
       " 'fake',\n",
       " 'fakin',\n",
       " 'faking',\n",
       " 'fall',\n",
       " 'fallin',\n",
       " 'falling',\n",
       " 'fallout',\n",
       " 'falls',\n",
       " 'fame',\n",
       " 'familiar',\n",
       " 'family',\n",
       " 'fancy',\n",
       " 'fantasy',\n",
       " 'far',\n",
       " 'farm',\n",
       " 'fast',\n",
       " 'faster',\n",
       " 'fatal',\n",
       " 'fate',\n",
       " 'fatefully',\n",
       " 'father',\n",
       " 'favorite',\n",
       " 'fear',\n",
       " 'fearless',\n",
       " 'fears',\n",
       " 'feel',\n",
       " 'feelin',\n",
       " 'feeling',\n",
       " 'feelings',\n",
       " 'feels',\n",
       " 'feet',\n",
       " 'fell',\n",
       " 'felt',\n",
       " 'fever',\n",
       " 'few',\n",
       " 'fields',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'fight',\n",
       " 'fighting',\n",
       " 'fights',\n",
       " 'figure',\n",
       " 'figured',\n",
       " 'final',\n",
       " 'finally',\n",
       " 'find',\n",
       " 'fine',\n",
       " 'fire',\n",
       " 'first',\n",
       " 'fist',\n",
       " 'fit',\n",
       " 'fits',\n",
       " 'five',\n",
       " 'fix',\n",
       " 'flame',\n",
       " 'flannel',\n",
       " 'flashback',\n",
       " 'flashbacks',\n",
       " 'flashed',\n",
       " 'flashes',\n",
       " 'flaw',\n",
       " 'flawless',\n",
       " 'flew',\n",
       " 'flickers',\n",
       " 'flies',\n",
       " 'flight',\n",
       " 'flights',\n",
       " 'floods',\n",
       " 'floor',\n",
       " 'floors',\n",
       " 'flow',\n",
       " 'flowers',\n",
       " 'flush',\n",
       " 'fly',\n",
       " 'flyin',\n",
       " 'flying',\n",
       " 'focus',\n",
       " 'fogs',\n",
       " 'folklore',\n",
       " 'folks',\n",
       " 'follow',\n",
       " 'fool',\n",
       " 'football',\n",
       " 'footnotes',\n",
       " 'footsteps',\n",
       " 'for',\n",
       " 'forcing',\n",
       " 'forever',\n",
       " 'forget',\n",
       " 'forgets',\n",
       " 'forgetting',\n",
       " 'forgive',\n",
       " 'forgiveness',\n",
       " 'forgot',\n",
       " 'forgotten',\n",
       " 'forth',\n",
       " 'fortune',\n",
       " 'forward',\n",
       " 'fought',\n",
       " 'found',\n",
       " 'four',\n",
       " 'foyer',\n",
       " 'fractured',\n",
       " 'frame',\n",
       " 'fray',\n",
       " 'freckle',\n",
       " 'freckles',\n",
       " 'free',\n",
       " 'freedom',\n",
       " 'freezing',\n",
       " 'freshman',\n",
       " 'friction',\n",
       " 'friend',\n",
       " 'friends',\n",
       " 'frightenin',\n",
       " 'from',\n",
       " 'front',\n",
       " 'frost',\n",
       " 'frozen',\n",
       " 'frustrating',\n",
       " 'frustratеd',\n",
       " 'fuck',\n",
       " 'fucked',\n",
       " 'full',\n",
       " 'fun',\n",
       " 'funny',\n",
       " 'fury',\n",
       " 'future',\n",
       " 'fеarless',\n",
       " 'gain',\n",
       " 'game',\n",
       " 'games',\n",
       " 'garden',\n",
       " 'gardens',\n",
       " 'gardеn',\n",
       " 'gate',\n",
       " 'gated',\n",
       " 'gates',\n",
       " 'gave',\n",
       " 'gaze',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'gift',\n",
       " 'girl',\n",
       " 'girlfriend',\n",
       " 'girls',\n",
       " 'give',\n",
       " 'given',\n",
       " 'giving',\n",
       " 'glamorous',\n",
       " 'glance',\n",
       " 'glass',\n",
       " 'glasses',\n",
       " 'gleam',\n",
       " 'gleaming',\n",
       " 'glimmer',\n",
       " 'glisten',\n",
       " 'glistened',\n",
       " 'glistеn',\n",
       " 'glorious',\n",
       " 'glow',\n",
       " 'go',\n",
       " 'god',\n",
       " 'goddamn',\n",
       " 'goes',\n",
       " 'goin',\n",
       " 'going',\n",
       " 'gold',\n",
       " 'golden',\n",
       " 'gone',\n",
       " 'gonna',\n",
       " 'good',\n",
       " 'goodbye',\n",
       " 'goodbyes',\n",
       " 'got',\n",
       " 'gotta',\n",
       " 'gowns',\n",
       " 'grab',\n",
       " 'grace',\n",
       " 'graffiti',\n",
       " 'grand',\n",
       " 'granted',\n",
       " 'grave',\n",
       " 'gravity',\n",
       " 'gravitys',\n",
       " 'gray',\n",
       " 'great',\n",
       " 'greater',\n",
       " 'greed',\n",
       " 'green',\n",
       " 'greet',\n",
       " 'grew',\n",
       " 'grey',\n",
       " 'grieving',\n",
       " 'grin',\n",
       " 'groceries',\n",
       " 'grocery',\n",
       " 'ground',\n",
       " 'group',\n",
       " 'grow',\n",
       " 'growin',\n",
       " 'growing',\n",
       " 'grows',\n",
       " 'guess',\n",
       " 'guilty',\n",
       " 'guitar',\n",
       " 'gun',\n",
       " 'guy',\n",
       " 'guys',\n",
       " 'ha',\n",
       " 'had',\n",
       " 'hadn',\n",
       " 'hadnt',\n",
       " 'hair',\n",
       " 'half',\n",
       " 'halfway',\n",
       " 'hall',\n",
       " 'hallelujah',\n",
       " 'hallelujahyou',\n",
       " 'halloween',\n",
       " 'halls',\n",
       " 'hallway',\n",
       " 'hand',\n",
       " 'handled',\n",
       " 'hands',\n",
       " 'handwritten',\n",
       " 'hang',\n",
       " 'hanging',\n",
       " 'happen',\n",
       " 'happened',\n",
       " 'happiness',\n",
       " 'happy',\n",
       " 'happyyou',\n",
       " 'hard',\n",
       " 'harder',\n",
       " 'harm',\n",
       " 'has',\n",
       " 'hasnt',\n",
       " 'hate',\n",
       " 'haul',\n",
       " 'haunt',\n",
       " 'haunted',\n",
       " 'have',\n",
       " 'havent',\n",
       " 'having',\n",
       " 'he',\n",
       " 'head',\n",
       " 'headlights',\n",
       " 'headphones',\n",
       " 'heal',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'hears',\n",
       " 'heart',\n",
       " 'heartbeat',\n",
       " 'heartbreaks',\n",
       " 'hearted',\n",
       " 'hearts',\n",
       " 'heat',\n",
       " 'heaven',\n",
       " 'hed',\n",
       " 'heel',\n",
       " 'heels',\n",
       " 'held',\n",
       " 'hell',\n",
       " 'hello',\n",
       " 'helmet',\n",
       " 'help',\n",
       " 'her',\n",
       " 'here',\n",
       " 'heres',\n",
       " 'heros',\n",
       " 'herе',\n",
       " 'hes',\n",
       " 'hesitation',\n",
       " 'hey',\n",
       " 'hi',\n",
       " 'hide',\n",
       " 'hiding',\n",
       " 'high',\n",
       " 'highest',\n",
       " 'hills',\n",
       " 'him',\n",
       " 'hip',\n",
       " 'hipsters',\n",
       " 'his',\n",
       " 'history',\n",
       " 'hits',\n",
       " 'hold',\n",
       " 'holdin',\n",
       " 'holding',\n",
       " 'hole',\n",
       " 'holidays',\n",
       " 'hollow',\n",
       " 'hollywood',\n",
       " 'holy',\n",
       " 'home',\n",
       " 'hometown',\n",
       " 'homе',\n",
       " 'honest',\n",
       " 'honestly',\n",
       " 'honey',\n",
       " 'hoo',\n",
       " 'hope',\n",
       " 'hopeful',\n",
       " 'hopelessly',\n",
       " 'hopes',\n",
       " 'hopin',\n",
       " 'hoping',\n",
       " 'horse',\n",
       " 'hospital',\n",
       " 'hotel',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'houses',\n",
       " 'how',\n",
       " 'howd',\n",
       " 'hows',\n",
       " 'hug',\n",
       " 'huh',\n",
       " 'humor',\n",
       " 'hundred',\n",
       " 'hunt',\n",
       " 'hurt',\n",
       " 'hurts',\n",
       " 'husbands',\n",
       " 'hustlin',\n",
       " 'hustling',\n",
       " 'hypnotized',\n",
       " 'hе',\n",
       " 'id',\n",
       " 'idea',\n",
       " 'if',\n",
       " 'ignorin',\n",
       " 'ill',\n",
       " 'im',\n",
       " 'ima',\n",
       " 'imagine',\n",
       " 'impossible',\n",
       " 'impress',\n",
       " 'in',\n",
       " 'incandescent',\n",
       " 'incredible',\n",
       " 'indie',\n",
       " 'indiscretions',\n",
       " 'indulgent',\n",
       " 'infidelity',\n",
       " 'ing',\n",
       " 'ingénue',\n",
       " 'innocence',\n",
       " 'innocent',\n",
       " 'insane',\n",
       " 'inside',\n",
       " 'insincere',\n",
       " 'insisting',\n",
       " 'instead',\n",
       " 'insurance',\n",
       " 'internet',\n",
       " 'into',\n",
       " 'intoxicating',\n",
       " 'invisible',\n",
       " 'inviting',\n",
       " 'iron',\n",
       " 'is',\n",
       " 'island',\n",
       " 'isnt',\n",
       " 'it',\n",
       " 'itll',\n",
       " 'its',\n",
       " 'ive',\n",
       " 'ivy',\n",
       " 'james',\n",
       " 'jealous',\n",
       " 'jealousy',\n",
       " 'jeans',\n",
       " 'jewel',\n",
       " 'jewelry',\n",
       " 'job',\n",
       " 'joint',\n",
       " 'joke',\n",
       " 'jokes',\n",
       " 'juliet',\n",
       " 'july',\n",
       " 'jump',\n",
       " 'jumping',\n",
       " 'june',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'keepin',\n",
       " 'keeping',\n",
       " 'kept',\n",
       " 'key',\n",
       " 'keychain',\n",
       " 'keys',\n",
       " 'kid',\n",
       " 'kidding',\n",
       " 'kids',\n",
       " 'killing',\n",
       " 'kind',\n",
       " ...]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfv.get_feature_names()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.12 (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
